import os
import json
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Union, List, Dict

class MFCC:
    """
    MyFirstCoolCodec (MFCC) —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π MP4
    """
    
    # === –ë–ê–ó–û–í–´–ï –§–£–ù–ö–¶–ò–ò ===
    @staticmethod
    def encode_nosplit(data: Union[bytes, str]) -> str:
        """–°–∂–∏–º–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏—Å–ø–æ–ª—å–∑—É—è RLE —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏"""
        if isinstance(data, str):
            data = data.encode('utf-8')
        
        hex_str = data.hex().upper()
        if not hex_str:
            return ""
        
        encoded = []
        count = 1
        current_char = hex_str[0]
        
        for i in range(1, len(hex_str)):
            if hex_str[i] == current_char and count < 255:
                count += 1
            else:
                if count > 3:
                    encoded.append(f"{count:02X}|{current_char}|")
                else:
                    encoded.append(current_char * count)
                count = 1
                current_char = hex_str[i]
        
        if count > 3:
            encoded.append(f"{count:02X}|{current_char}|")
        else:
            encoded.append(current_char * count)
            
        return "".join(encoded)
    
    @staticmethod
    def decode_nosplit(compressed_data: str) -> bytes:
        """–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ nosplit —Ñ–æ—Ä–º–∞—Ç–∞"""
        if not compressed_data:
            return b""
        
        decoded_hex = []
        i = 0
        
        while i < len(compressed_data):
            if (i + 4 <= len(compressed_data) and 
                compressed_data[i+2] == '|' and 
                compressed_data[i+4] == '|'):
                
                if (compressed_data[i] in '0123456789ABCDEF' and 
                    compressed_data[i+1] in '0123456789ABCDEF' and
                    compressed_data[i+3] in '0123456789ABCDEF'):
                    
                    try:
                        count = int(compressed_data[i:i+2], 16)
                        symbol = compressed_data[i+3]
                        decoded_hex.append(symbol * count)
                        i += 5
                        continue
                    except ValueError:
                        pass
            
            if compressed_data[i] in '0123456789ABCDEF':
                decoded_hex.append(compressed_data[i])
            i += 1
        
        hex_str = "".join(decoded_hex)
        if len(hex_str) % 2 != 0:
            hex_str += '0'
        
        return bytes.fromhex(hex_str)
    
    # === –ú–ù–û–ì–û–ü–û–¢–û–ß–ù–û–°–¢–¨ –î–õ–Ø –ë–û–õ–¨–®–ò–• –§–ê–ô–õ–û–í ===
    @staticmethod
    def encode_large_file_parallel(input_path: str, output_path: str, chunk_size=10*1024*1024, max_workers=4) -> bool:
        """–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            file_size = os.path.getsize(input_path)
            total_chunks = (file_size + chunk_size - 1) // chunk_size
            
            print(f"üîß –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–µ —Å–∂–∞—Ç–∏–µ: {file_size/(1024*1024):.1f} MB")
            print(f"üì¶ –ß–∞–Ω–∫–æ–≤: {total_chunks}, –ü–æ—Ç–æ–∫–æ–≤: {max_workers}")
            
            results = []
            lock = threading.Lock()
            
            def process_chunk(chunk_id, start_pos, chunk_size):
                try:
                    with open(input_path, 'rb') as f:
                        f.seek(start_pos)
                        chunk_data = f.read(chunk_size)
                    
                    compressed = MFCC.encode_nosplit(chunk_data)
                    
                    with lock:
                        results.append((chunk_id, compressed))
                    
                    return True
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —á–∞–Ω–∫–µ {chunk_id}: {e}")
                    return False
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = []
                for i in range(total_chunks):
                    start_pos = i * chunk_size
                    actual_chunk_size = min(chunk_size, file_size - start_pos)
                    future = executor.submit(process_chunk, i, start_pos, actual_chunk_size)
                    futures.append(future)
                
                # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                completed = 0
                for future in as_completed(futures):
                    completed += 1
                    if completed % 10 == 0:
                        print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {completed}/{total_chunks} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results.sort(key=lambda x: x[0])
            
            with open(output_path, 'w', encoding='utf-8') as f:
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
                metadata = {
                    "format": "MFCC_PARALLEL",
                    "chunks": total_chunks,
                    "original_size": file_size
                }
                f.write(json.dumps(metadata) + "\n")
                
                for chunk_id, compressed in results:
                    f.write(f"{compressed}\n")
            
            print(f"‚úÖ –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {output_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è: {e}")
            return False
    
    @staticmethod
    def decode_large_file_parallel(input_path: str, output_path: str, max_workers=4) -> bool:
        """–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            if not lines or len(lines) < 2:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
                return False
            
            # –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = json.loads(lines[0].strip())
            total_chunks = metadata["chunks"]
            
            print(f"üîß –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞: {total_chunks} —á–∞–Ω–∫–æ–≤")
            
            if len(lines) != total_chunks + 1:
                print("‚ùå –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤")
                return False
            
            results = []
            lock = threading.Lock()
            
            def process_chunk(chunk_id, compressed_data):
                try:
                    decoded = MFCC.decode_nosplit(compressed_data)
                    
                    with lock:
                        results.append((chunk_id, decoded))
                    
                    return True
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ —á–∞–Ω–∫–∞ {chunk_id}: {e}")
                    return False
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏ –¥–ª—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = []
                for i in range(1, len(lines)):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    future = executor.submit(process_chunk, i-1, lines[i].strip())
                    futures.append(future)
                
                completed = 0
                for future in as_completed(futures):
                    completed += 1
                    if completed % 10 == 0:
                        print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {completed}/{total_chunks} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ —Å–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            results.sort(key=lambda x: x[0])
            
            with open(output_path, 'wb') as f:
                for chunk_id, data in results:
                    f.write(data)
            
            print(f"‚úÖ –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {output_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–π —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏: {e}")
            return False
    
    # === –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê MP4 ===
    @staticmethod
    def encode_mp4(input_path: str, output_path: str) -> bool:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ MP4 —Ñ–∞–π–ª–æ–≤"""
        try:
            file_size = os.path.getsize(input_path)
            print(f"üé• –û–±—Ä–∞–±–æ—Ç–∫–∞ MP4: {file_size/(1024*1024):.1f} MB")
            
            # –î–ª—è MP4 –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            return MFCC.encode_large_file_parallel(
                input_path, 
                output_path, 
                chunk_size=5*1024*1024,  # 5MB —á–∞–Ω–∫–∏ –¥–ª—è MP4
                max_workers=2  # –ú–µ–Ω—å—à–µ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            )
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ MP4: {e}")
            return False
    
    @staticmethod
    def decode_mp4(input_path: str, output_path: str) -> bool:
        """–†–∞—Å–ø–∞–∫–æ–≤–∫–∞ MP4 —Ñ–∞–π–ª–æ–≤"""
        try:
            return MFCC.decode_large_file_parallel(input_path, output_path, max_workers=2)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ MP4: {e}")
            return False
    
    # === –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –†–ï–ñ–ò–ú–ê ===
    @staticmethod
    def encode_file_auto(input_path: str, output_path: str) -> bool:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è"""
        file_size = os.path.getsize(input_path)
        file_ext = os.path.splitext(input_path)[1].lower()
        
        # –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã (>900MB) - –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å
        if file_size > 900 * 1024 * 1024:
            print("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –±–æ–ª—å—à–æ–≥–æ —Ñ–∞–π–ª–∞")
            return MFCC.encode_large_file_parallel(input_path, output_path)
        
        # MP4 —Ñ–∞–π–ª—ã - —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        elif file_ext == '.mp4':
            print("üé• –ò—Å–ø–æ–ª—å–∑—É–µ–º MP4 —Ä–µ–∂–∏–º")
            return MFCC.encode_mp4(input_path, output_path)
        
        # –û–±—ã—á–Ω—ã–µ —Ñ–∞–π–ª—ã - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥
        else:
            print("üìÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º")
            return MFCC.encode_file_nosplit(input_path, output_path)
    
    @staticmethod
    def decode_file_auto(input_path: str, output_path: str) -> bool:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if first_line.startswith('{"format": "MFCC_PARALLEL"'):
                return MFCC.decode_large_file_parallel(input_path, output_path)
            else:
                return MFCC.decode_file_nosplit(input_path, output_path)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥")
            return MFCC.decode_file_nosplit(input_path, output_path)
    
    # === –°–¢–ê–ù–î–ê–†–¢–ù–´–ï –ú–ï–¢–û–î–´ ===
    @staticmethod
    def encode_file_nosplit(input_path: str, output_path: str) -> bool:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            with open(input_path, 'rb') as f:
                data = f.read()
            
            compressed = MFCC.encode_nosplit(data)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(compressed)
            
            print(f"‚úÖ –°–∂–∞—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {input_path} ‚Üí {output_path}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∂–∞—Ç–∏—è: {e}")
            return False
    
    @staticmethod
    def decode_file_nosplit(input_path: str, output_path: str) -> bool:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                compressed_data = f.read()
            
            decoded_data = MFCC.decode_nosplit(compressed_data)
            
            with open(output_path, 'wb') as f:
                f.write(decoded_data)
            
            print(f"‚úÖ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {input_path} ‚Üí {output_path}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏: {e}")
            return False
